{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Get TFIDF of each Product Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text  import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_tfidf(product_details):\n",
    "    clean_product = []\n",
    "    product_name = list(product_details)\n",
    "    for i in range(len(product_name)):\n",
    "        words = \"\"\n",
    "\n",
    "        doc = nlp(product_name[i].lower())\n",
    "        for token in doc:\n",
    "            token.lemma_ = re.sub(r'\\W',' ',token.lemma_)\n",
    "            token.lemma_ = token.lemma_.strip()\n",
    "            if not token.lemma_.endswith(\"ml\") and not token.lemma_.endswith(\"ms\") and not token.lemma_.isdigit() and not token.lemma_ in stop_words:\n",
    "                if len(token.lemma_) > 2 or token.lemma_ == 'uv': \n",
    "                    words += token.lemma_.lower() + \" \"\n",
    "                    \n",
    "\n",
    "        if len(words) > 0:\n",
    "            clean_product.append(str(words.strip()))\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(clean_product)\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), \n",
    "                      columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"], ascending=False).reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "entries = os.listdir('dataset/updated/') ## Please Change Directory\n",
    "main_category_list = []\n",
    "sub_category_list = []\n",
    "sub_category_list_2 = []\n",
    "product_name_list = []\n",
    "\n",
    "dataset  = pd.read_csv('dataset/{}.csv'.format(category), error_bad_lines=False)  ## Please Change Directory\n",
    "dataset['Main Category'] = dataset['Main Category'].str.lower()\n",
    "dataset['Sub Category 1'] = dataset['Sub Category 1'].str.lower()\n",
    "dataset['Sub Category 2'] = dataset['Sub Category 2'].str.lower()\n",
    "\n",
    "main_category = dataset['Main Category'].unique()\n",
    "for _main_category in main_category:\n",
    "    if type(_main_category) == str:\n",
    "        print(_main_category)\n",
    "        main_category_data = dataset[dataset['Main Category'] == _main_category]\n",
    "        G.add_node(_main_category)\n",
    "        main_category_list.append(_main_category)\n",
    "        \n",
    "        for row in main_category_data['Sub Category 1'].unique():\n",
    "            if type(row) == str:\n",
    "                sub_category_list.append(row)\n",
    "                G.add_edge(row, _main_category, weight=1.0)\n",
    "                \n",
    "                for row2 in main_category_data.loc[main_category_data['Sub Category 1'] == row]['Sub Category 2'].unique():\n",
    "                    if type(row2) == str:\n",
    "                        G.add_edge(row2.strip(), row.strip(), weight=1.0)\n",
    "                        sub_category_list_2.append(row2.strip())\n",
    "                        \n",
    "                        tfidf_result = get_tfidf(main_category_data.loc[main_category_data['Sub Category 2'] == row2]['Product Name'])\n",
    "                        \n",
    "                        index =  tfidf_result['index']\n",
    "                        tfidf = tfidf_result['tfidf']\n",
    "                        counter = 0\n",
    "                        for _tfidf_result in tfidf_result['index']:\n",
    "                            if float(tfidf[counter]) > 0.0:                    \n",
    "                                if index[counter] not in main_category_list:\n",
    "                                    product_name_list.append(index[counter].lower())\n",
    "                                    G.add_edge(index[counter].lower(), row2.strip(), weight=tfidf[counter])\n",
    "    \n",
    "    \n",
    "                            counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Network Theory to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('network_theory.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(G, fe_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETWEENNESS_CENTRALITY = nx.betweenness_centrality(G)\n",
    "\n",
    "with open('betweeness_centrality.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(G, fe_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Neighbors and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(given_wishlist.strip())\n",
    "\n",
    "result_categories = []\n",
    "\n",
    "for token in reversed(doc):\n",
    "    if token.text in list(G.nodes()):\n",
    "        print(token.lemma_)\n",
    "        closeness_centrality_list = []\n",
    "        betweness_centrality_list = []\n",
    "        degree_list = []\n",
    "        neighbor_list = []\n",
    "        shortest_path_list = []\n",
    "        length_list = []\n",
    "\n",
    "        for _neighbors in list(G.neighbors(token.text)):\n",
    "            if _neighbors in sub_category_list_2:\n",
    "                neighbor_list.append(_neighbors)\n",
    "                betweness_centrality_list.append(between_centrality_json[_neighbors])\n",
    "                shortest_path = nx.shortest_path(G, source=_neighbors, target=token.lemma_)\n",
    "                shortest_path_list.append(len(shortest_path))\n",
    "                length_list.append(overall_data.loc[overall_data['Sub Category 2'] == _neighbors].shape[0])\n",
    "\n",
    "        network_result = pd.DataFrame(neighbor_list, columns=['neighbor'])\n",
    "        network_result['betweeness_centrality'] = betweness_centrality_list\n",
    "        network_result['shortest_path'] = shortest_path_list\n",
    "\n",
    "        if len(betweness_centrality_list) > 0:\n",
    "            if network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'].shape[0] < 2:\n",
    "                if list(network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'])[0] not in result_categories:\n",
    "                    result_categories.append(list(network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'])[0])\n",
    "            else:\n",
    "                if list(network_result[network_result['betweeness_centrality'] == min(betweness_centrality_list)]['neighbor'])[0] not in result_categories:\n",
    "                    result_categories.append(list(network_result[network_result['betweeness_centrality'] == min(betweness_centrality_list)]['neighbor'])[0]) \n",
    "merge_products = []\n",
    "for _result_categories in result_categories:\n",
    "    merge_products.append(overall_data.loc[(overall_data['Sub Category 2'] == _result_categories.title())])\n",
    "    \n",
    "selected_category = pd.concat(merge_products).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Relevance Per Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "sample_wishlist = ['jogger pants']\n",
    "\n",
    "vectorize = TfidfVectorizer(stop_words='english')\n",
    "tfidf_response= vectorize.fit_transform(selected_category['Product Name'])\n",
    "dtm = pd.DataFrame(tfidf_response.todense(), columns = vectorize.get_feature_names())\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=selected_category.shape[0])\n",
    "nn.fit(dtm)\n",
    "\n",
    "new = vectorize.transform(sample_wishlist)\n",
    "knn_model_result = nn.kneighbors(new.todense())\n",
    "\n",
    "knn_result = pd.DataFrame(knn_model_result[0][0][0:], columns=['Distance'])\n",
    "knn_result[\"Product Name\"] = selected_category['Product Name'][knn_model_result[1][0][0:]]\n",
    "\n",
    "merged_result = pd.merge(selected_category, knn_result, on='Product Name', how='inner')\n",
    "merged_result = merged_result.drop_duplicates(subset='Product Name', keep=\"first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
