{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T06:22:06.539942Z",
     "start_time": "2020-11-24T06:22:00.103634Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from IPython import get_ipython\n",
    "from PIL import Image\n",
    "from streamlit import caching\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text  import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import streamlit.components.v1 as components\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import base64\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "from streamlit_embedcode import github_gist\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "with open(\"style_img.css\") as f:\n",
    "    st.markdown('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)\n",
    "image = Image.open('img/santa.png')\n",
    "st.image(image, width = 500)\n",
    "\n",
    "image = Image.open('img/eskwelabs_logo.jpg')\n",
    "st.sidebar.image(image, caption='', use_column_width=True)\n",
    "\n",
    "add_selectbox = st.sidebar.radio(\n",
    "    \"\",\n",
    "    (\"Introduction and Problem Statement\", \"List of Tools\", \"Data Sourcing\", \"Feature Engineering\", \n",
    "     \"Exploratory Data Analysis\", \"Recommender Engine\", \n",
    "     \"Recommendations\", \"Contributors\")\n",
    ")\n",
    "\n",
    "if add_selectbox == 'Introduction and Problem Statement':\n",
    "    st.write('')\n",
    "    \n",
    "    image = Image.open('img/intro_img.png').convert('RGB')\n",
    "    st.image(image, caption='Source:https://unsplash.com/photos/2plsuhgV53I', width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    Gift giving during the holiday season is inherent in Filipinos and it is traditionally coupled with holiday \n",
    "    shopping in malls and other offline stores. But because of the pandemic, majority of this traditionally offline \n",
    "    ritual will be happening online and people will be looking to e-commerce marketplaces to make their holiday shopping \n",
    "    easier and safer.\n",
    "    <br>\n",
    "    <br>\n",
    "    Our project aims to create a feature that will support the gift-giving activities from consumers in the form of gift \n",
    "    recommendations and public wishlists supplemented with features that will help users build wishlists. Having this \n",
    "    feature that supports this already inherent behavior will give an e-commerce player an advantage over competitors by \n",
    "    attracting shoppers to go to their platform due to ease and convenience instead of the competitorsâ€™.\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "elif add_selectbox == 'List of Tools':\n",
    "    st.subheader('List of Tools')\n",
    "    \n",
    "    st.write('___') \n",
    "    \n",
    "    st.write('''**Integrated Development Environment:**''', unsafe_allow_html=True)\n",
    "    image = Image.open('img/anaconda.png').convert('RGB')\n",
    "    st.image(image, caption='Anaconda', width=300, height=150)\n",
    "    image = Image.open('img/jupyter.png').convert('RGB')\n",
    "    st.image(image, caption='Jupyter Notebook', width=300, height=150)\n",
    "    \n",
    "    st.write('''<br>**Main Programming Language:**''', unsafe_allow_html=True)\n",
    "    image = Image.open('img/python.png').convert('RGB')\n",
    "    st.image(image, caption='Python', width=300, height=150)\n",
    "    \n",
    "    st.write('''<br>**Data Visualization and Exploratory Data Analysis:**''', unsafe_allow_html=True)\n",
    "    image = Image.open('img/pandas.png').convert('RGB')\n",
    "    st.image(image, caption='Pandas', width=300, height=150)\n",
    "    image = Image.open('img/seaborn.png').convert('RGB')\n",
    "    st.image(image, caption='Seaborn', width=300, height=150)\n",
    "    image = Image.open('img/matplotlib.png').convert('RGB')\n",
    "    st.image(image, caption='Matplotlib', width=300, height=150)\n",
    "    \n",
    "    st.write('''<br>**Modeling:**''', unsafe_allow_html=True)\n",
    "    image = Image.open('img/spacy.png').convert('RGB')\n",
    "    st.image(image, caption='spaCy', width=300, height=150)\n",
    "    image = Image.open('img/networkx.png').convert('RGB')\n",
    "    st.image(image, caption='NetworkX', width=300, height=150)\n",
    "    image = Image.open('img/sklearn.png').convert('RGB')\n",
    "    st.image(image, caption='Scikit-learn', width=300, height=150)\n",
    "    \n",
    "    st.write('''<br>**Deployment:**''', unsafe_allow_html=True)\n",
    "    image = Image.open('img/streamlit.png').convert('RGB')\n",
    "    st.image(image, caption='Streamlit', width=300, height=150)\n",
    "    image = Image.open('img/heroku.png').convert('RGB')\n",
    "    st.image(image, caption='Heroku', width=300, height=150)\n",
    "\n",
    "elif add_selectbox == 'Data Sourcing':\n",
    "    st.subheader('Data Sourcing')\n",
    "    \n",
    "    shopee_links = pd.read_csv('data/Mobiles-Gadgets-cat.24456_links.csv', error_bad_lines=False)\n",
    "    shopee_data = pd.read_csv('data/Mobiles-Gadgets-cat.24456.csv', error_bad_lines=False)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    For this project, our group chose to scrape product data from one of the Philippines' leading e-commerce platforms, Shopee.\n",
    "    Since Shopee utilizes Lazy Loading, we used BeautifulSoup together with Selenium to automate the data scraping process.\n",
    "    <br>\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    image = Image.open('img/main_categories.png').convert('RGB')\n",
    "    st.image(image, caption='Shopee Main Categories', height=300, width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    The first step in our data scraping process was to make sure that all products of each Main Category in Shopee are represented\n",
    "    equally. For this to happen, we needed to scrape all the URL link of each product. The image below shows a sample of the automated process\n",
    "    of scraping URL links from each product in Mobiles & Gadgets Category. Our group repeated this process for all other categories in Shopee.\n",
    "    <br>\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.image('img/shopee_link.gif', width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    <br>\n",
    "    After the scraping process for URL links, a CSV file will be created with the following contents.\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "    st.table(shopee_links.head(1))\n",
    "    \n",
    "    st.write('<b>Main Category List:</b>', unsafe_allow_html=True)\n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Babies & Kids</li>\"\\\n",
    "                \"<li>Cameras</li>\"\n",
    "                \"<li>Digital Goods and Vouchers</li>\"\\\n",
    "                \"<li>Gaming</li>\"\\\n",
    "                \"<li>Groceries</li>\"\\\n",
    "                \"<li>Health and Personal Care</li>\"\\\n",
    "                \"<li>Hobbies and Stationery</li>\"\\\n",
    "                \"<li>Home and Living</li>\"\\\n",
    "                \"<li>Home Appliances</li>\"\\\n",
    "                \"<li>Home Entertainment</li>\"\\\n",
    "                \"<li>Laptops and Computers</li>\"\\\n",
    "                \"<li>Makeup and Fragrances</li>\"\\\n",
    "                \"<li>Men's Apparel</li>\"\\\n",
    "                \"<li>Men's Bags and Accessories</li>\"\\\n",
    "                \"<li>Men's Shoes</li>\"\\\n",
    "                \"<li>Mobile Accessories</li>\"\\\n",
    "                \"<li>Mobiles and Gadgets</li>\"\\\n",
    "                \"<li>Motors</li>\"\\\n",
    "                \"<li>Pet Care</li>\"\\\n",
    "                \"<li>Sports and Travel</li>\"\\\n",
    "                \"<li>Toys, Games & Collectibles</li>\"\\\n",
    "                \"<li>Women's Accessories</li>\"\\\n",
    "                \"<li>Women's Apparel</li>\"\\\n",
    "                \"<li>Women's Bags</li>\"\\\n",
    "                \"<li>Women's Shoes</li>\"\\\n",
    "                 \"</ul>\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    After scraping each product URL, we then proceed to the product details. Our group again used BeautifulSoup and Selenium to scrape \n",
    "    each product details that we have deemed useful in building our recommender engine. \n",
    "    <br>\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.image('img/shopee_detail.gif', width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    <br>\n",
    "    Shown below is the CSV file that was created after scraping the details of each product under the category\n",
    "    for Men's Bags & Accessories.\n",
    "    <br>\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "    st.dataframe(shopee_data, width=680)\n",
    "    \n",
    "elif add_selectbox == 'Feature Engineering':\n",
    "    st.subheader('Feature Engineering')\n",
    "    \n",
    "    DATA_URL = ('data/cleaned_data_2000 (11.23.2020).csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        overall_data = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        return overall_data\n",
    "    overall_data = load_data(101)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    After scraping all product details in each category, we now proceed in merging it all in to one CSV file. We used the code\n",
    "    below to do this.\n",
    "    <br>\n",
    "    <br>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    github_gist(\"https://gist.github.com/robibanadera/f591993a9fbbe0777c156a3177fe5664/\",height=300, width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    After this process, we can now proceed with data cleaning and feature engineering. As we all know, data cleaning is an\n",
    "    important part of data preprocessing. It lets data scientist improve the results of their findings by removing or\n",
    "    modifying data that is incorrect, incomplete, irrelevant, duplicated, or improperly formatted.\n",
    "    <br>\n",
    "    <br>\n",
    "    For this dataset, we start this off by removing duplicated rows and also rows that have null values in the 'Product Name'\n",
    "    column. We also chose to drop some columns that our group deemed to have little or no importance in the goal that we are\n",
    "    trying to achieve. Through this process, we would have rows that have complete values for each column thus leaving us \n",
    "    with a more robust dataset.\n",
    "    <br>\n",
    "    <br>\n",
    "    The next step would be cleaning up the data from columns having unneeded characters and/or repetitive symbols \n",
    "    (eg. Currency $). This process is importance especially for columns dealing with numbers but then can't be processed by\n",
    "    Python because of the values being a string in nature rather than an integer. We used the code embedded below to accomplish\n",
    "    this step.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    github_gist(\"https://gist.github.com/robibanadera/fc4c21d81956fdd0759cbcd987553187/\",height=300, width=680)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    After removing unnecessary symbols/character in numerical columns, we now proceed to converting it to integer or float\n",
    "    values. We also checked the data types for the other columns if it is correct so no further problems will occur during coding\n",
    "    in the future.\n",
    "    <br>\n",
    "    <br>\n",
    "    You can now see below the sample for final dataset that we will use moving forward.\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    overall_data = overall_data.drop(['Description'], axis=1)\n",
    "    st.dataframe(overall_data, width=680)\n",
    "    st.write(\"\"\"\n",
    "    <b>Dataset Shape:</b>\n",
    "    <br>\n",
    "    Rows: 20918\n",
    "    <br>\n",
    "    Columns: 42\n",
    "    <br>\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write('<b>Dataset Features:</b>', unsafe_allow_html=True)\n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Product ID</li>\"\\\n",
    "                \"<li>URL</li>\"\n",
    "                \"<li>Page</li>\"\\\n",
    "                \"<li>Preferred</li>\"\\\n",
    "                \"<li>Mall</li>\"\\\n",
    "                \"<li>Product Name</li>\"\\\n",
    "                \"<li>Main Category</li>\"\\\n",
    "                \"<li>Sub Category 1</li>\"\\\n",
    "                \"<li>Sub Category 2</li>\"\\\n",
    "                \"<li>Current Rating</li>\"\\\n",
    "                \"<li>Total Rating</li>\"\\\n",
    "                \"<li>Total Sold</li>\"\\\n",
    "                \"<li>Favorite</li>\"\\\n",
    "                \"<li>Discount Range</li>\"\\\n",
    "                \"<li>Price Range</li>\"\\\n",
    "                \"<li>Discount Percentage</li>\"\\\n",
    "                \"<li>Free Shipping</li>\"\\\n",
    "                \"<li>Free Shipping Info</li>\"\\\n",
    "                \"<li>Shipping Location</li>\"\\\n",
    "                \"<li>Shipping Price Range</li>\"\\\n",
    "                \"<li>Brand Name</li>\"\\\n",
    "                \"<li>Store Name</li>\"\\\n",
    "                \"<li>Store Ratings</li>\"\\\n",
    "                \"<li>Store Products Count</li>\"\\\n",
    "                \"<li>Store Response Rate</li>\"\\\n",
    "                \"<li>Store Response Time</li>\"\\\n",
    "                \"<li>Store Joined</li>\"\\\n",
    "                \"<li>Store Followers</li>\"\\\n",
    "                \"<li>Shipping From</li>\"\\\n",
    "                \"<li>Vouchers Available</li>\"\\\n",
    "                \"<li>Bundle Details</li>\"\\\n",
    "                \"<li>Coins Available</li>\"\\\n",
    "                \"<li>Product Variation List</li>\"\\\n",
    "                \"<li>Lowest Price Guarantee</li>\"\\\n",
    "                \"<li>Whole Sale</li>\"\\\n",
    "                \"<li>Five Star</li>\"\\\n",
    "                \"<li>Four Star</li>\"\\\n",
    "                \"<li>Three Star</li>\"\\\n",
    "                \"<li>Two Star</li>\"\\\n",
    "                \"<li>One Star</li>\"\\\n",
    "                \"<li>With Comments</li>\"\\\n",
    "                \"<li>With Media</li>\"\\\n",
    "                 \"</ul>\", unsafe_allow_html=True)\n",
    "    \n",
    "elif add_selectbox == 'Exploratory Data Analysis':\n",
    "    st.subheader('Exploratory Data Analysis')\n",
    "    \n",
    "    df = pd.read_csv('data/cleaned_data_2000 (11.23.2020).csv', index_col=0)\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    Rating = ['Five Star', 'Four Star', 'Three Star', 'Two Star', 'One Star']\n",
    "    Count = list(df[['Five Star', 'Four Star', 'Three Star', 'Two Star', 'One Star' ]].sum())\n",
    "    plt.bar(Rating, Count)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    The bar graph above shows the distribution of the rating. We can observe that the distribution of the data \n",
    "    in the current ratings is skewed to five stars. This shows that Shopee users tend to be generous in \n",
    "    providing their ratings to their purchased products from Shopee.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = sns.kdeplot(df['Current Rating'], shade=True, color='orange')\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    Looking at the distribution of the average rating of the products, we can observe that it is skewed as \n",
    "    expected. It validates our first observation that SHoppee users are very generous in proviing ratings to \n",
    "    their purchased products.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = sns.kdeplot(df['Total Rating'], shade=True, color='orange')\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    Plotting the distribution of the total ratings of each product, we can see that most of the products \n",
    "    receive around 500 to 2000 reviews. In addition, there are very few prodjucts with more than 20,000 \n",
    "    total reviews.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = sns.kdeplot(df['Favorite'], shade=True, color='orange')\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    Plotting the distribution of the total favorites of each product, we can see that most of \n",
    "    the products receive around 500 to 2000 reviews. In addition, there are very few prodjucts with more \n",
    "    than 10,000 total reviews.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    included = ['Preferred','Mall', 'Current Rating', 'Total Sold', 'Favorite', 'Lowest Price Guarantee']\n",
    "    df1 = df[included]\n",
    "    corr = df1.corr()\n",
    "    fig = plt.figure(figsize=(13, 10))\n",
    "    ax = sns.heatmap(corr, annot=True, center=0)\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    From the correlation heatmap, we can observe that most of the features are not highly correlated to \n",
    "    each other except total sold and favorite features with a correlatio of 0.78.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    From the word cloud below we generated from the product names, we can see that most of the products \n",
    "    contains the words kids, women, old, pajama and terno the most.\n",
    "    <br>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Babies & Kids.png').convert('RGB')\n",
    "    st.image(image, caption='Babies & Kids', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "\n",
    "    image = Image.open('img/wordcloud/Cameras.png').convert('RGB')\n",
    "    st.image(image, caption='Cameras', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Gaming.png').convert('RGB')\n",
    "    st.image(image, caption='Gaming', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Health & Personal Care.png').convert('RGB')\n",
    "    st.image(image, caption='Health & Personal Care', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Hobbies & Stationery.png').convert('RGB')\n",
    "    st.image(image, caption='Hobbies & Stationery', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Home & Living.png').convert('RGB')\n",
    "    st.image(image, caption='Home & Living', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Home Appliances.png').convert('RGB')\n",
    "    st.image(image, caption='Home Appliances', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Home Entertainment.png').convert('RGB')\n",
    "    st.image(image, caption='Home Entertainment', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Laptops & Computers.png').convert('RGB')\n",
    "    st.image(image, caption='Laptops & Computers', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/wordcloud/Makeup & Fragrances.png').convert('RGB')\n",
    "    st.image(image, caption='Makeup & Fragrances', width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Men's Apparel.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Men's Apparel\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Men's Bags & Accessories.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Men's Bags & Accessories\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Men's Shoes.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Men's Shoes\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Mobiles & Gadgets.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Mobiles & Gadgets\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Motors.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Motors\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Pet Care.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Pet Care\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Sports & Travel.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Sports & Travel\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Women's Accessories.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Women's Accessories\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Women's Apparel.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Women's Apparel\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Women's Bags.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Women's Bags\", width=680)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open(\"img/wordcloud/Women's Bags.png\").convert('RGB')\n",
    "    st.image(image, caption=\"Women's Bags\", width=680)\n",
    "    \n",
    "elif add_selectbox == 'Recommender Engine':\n",
    "    \n",
    "    st.subheader('Recommender Engine')\n",
    "    \n",
    "    overall_data = pd.read_csv('data/cleaned_data_2000 (11.23.2020).csv', error_bad_lines=False)\n",
    "    st.write(\"What's your wish?\")\n",
    "    \n",
    "    sub_category_list_2 = list(overall_data['Sub Category 2'].str.lower().unique())\n",
    "    \n",
    "    with open('network_theory.pickle','rb') as fe_data_file:\n",
    "         G = pickle.load(fe_data_file)\n",
    "\n",
    "    with open('betweenness_centraility.json') as f:\n",
    "         between_centrality_json = json.load(f)\n",
    "    \n",
    "    def local_css(file_name):\n",
    "        with open(file_name) as f:\n",
    "            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)\n",
    "\n",
    "    def icon(icon_name):\n",
    "        st.markdown(f'<i class=\"material-icons\">{icon_name}</i>', unsafe_allow_html=True)\n",
    "\n",
    "    local_css(\"style.css\")\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def get_tfidf(product_details):\n",
    "        clean_product = []\n",
    "        product_name = list(product_details)\n",
    "        for i in range(len(product_name)):\n",
    "            words = \"\"\n",
    "\n",
    "            doc = nlp(product_name[i].lower())\n",
    "            for token in doc:\n",
    "                token.lemma_ = re.sub(r'\\W',' ',token.lemma_)\n",
    "                token.lemma_ = token.lemma_.strip()\n",
    "                if not token.lemma_.endswith(\"ml\") and not token.lemma_.endswith(\"ms\") and not token.lemma_.isdigit() and not token.lemma_ in stop_words:\n",
    "                    if len(token.lemma_) > 2 or token.lemma_ == 'uv': \n",
    "                        words += token.lemma_.lower() + \" \"\n",
    "                    \n",
    "\n",
    "            if len(words) > 0:\n",
    "                clean_product.append(str(words.strip()))\n",
    "\n",
    "        tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "        tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(clean_product)\n",
    "        first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "\n",
    "        df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), \n",
    "                      columns=[\"tfidf\"]) \n",
    "        df = df.sort_values(by=[\"tfidf\"], ascending=False).reset_index()\n",
    "    \n",
    "        return df\n",
    "\n",
    "    user_input = st.text_area(\"What are you looking for?\")\n",
    "    if st.button('Search'):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(user_input.strip())\n",
    "\n",
    "        result_categories = []\n",
    "\n",
    "        for token in reversed(doc):\n",
    "            if token.text in list(G.nodes()):\n",
    "                closeness_centrality_list = []\n",
    "                betweness_centrality_list = []\n",
    "                degree_list = []\n",
    "                neighbor_list = []\n",
    "                shortest_path_list = []\n",
    "                length_list = []\n",
    "\n",
    "                for _neighbors in list(G.neighbors(token.text)):\n",
    "                    if _neighbors in sub_category_list_2:\n",
    "                        neighbor_list.append(_neighbors)\n",
    "                        betweness_centrality_list.append(between_centrality_json[_neighbors])\n",
    "                        shortest_path = nx.shortest_path(G, source=_neighbors, target=token.lemma_)\n",
    "                        shortest_path_list.append(len(shortest_path))\n",
    "                        length_list.append(overall_data.loc[overall_data['Sub Category 2'] == _neighbors].shape[0])\n",
    "\n",
    "                network_result = pd.DataFrame(neighbor_list, columns=['neighbor'])\n",
    "                network_result['betweeness_centrality'] = betweness_centrality_list\n",
    "                network_result['shortest_path'] = shortest_path_list\n",
    "\n",
    "                if len(betweness_centrality_list) > 0:\n",
    "                    if network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'].shape[0] < 2:\n",
    "                        if list(network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'])[0] not in result_categories:\n",
    "                            result_categories.append(list(network_result[network_result['shortest_path'] == min(shortest_path_list)]['neighbor'])[0])\n",
    "                    else:\n",
    "                        if list(network_result[network_result['betweeness_centrality'] == min(betweness_centrality_list)]['neighbor'])[0] not in result_categories:\n",
    "                            result_categories.append(list(network_result[network_result['betweeness_centrality'] == min(betweness_centrality_list)]['neighbor'])[0]) \n",
    "        merge_products = []\n",
    "        for _result_categories in result_categories:\n",
    "            merge_products.append(overall_data.loc[(overall_data['Sub Category 2'] == _result_categories.title())])\n",
    "\n",
    "        selected_category = pd.concat(merge_products).reset_index()\n",
    "        \n",
    "        vectorize = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_response= vectorize.fit_transform(selected_category['Product Name'])\n",
    "        dtm = pd.DataFrame(tfidf_response.todense(), columns = vectorize.get_feature_names())\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=selected_category.shape[0])\n",
    "        nn.fit(dtm)\n",
    "        wishlist = [user_input]\n",
    "\n",
    "        new = vectorize.transform(wishlist)\n",
    "        knn_model_result = nn.kneighbors(new.todense())\n",
    "\n",
    "        knn_result = pd.DataFrame(knn_model_result[0][0][0:], columns=['Distance'])\n",
    "        knn_result[\"Product Name\"] = selected_category['Product Name'][knn_model_result[1][0][0:]]\n",
    "\n",
    "        merged_result = pd.merge(selected_category, knn_result, on='Product Name', how='inner')\n",
    "        merged_result = merged_result.drop_duplicates(subset='Product Name', keep=\"first\")\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        scoring_criteria = ['Trusted', 'Highly Rated', 'Discounted', 'Top Selling', 'High Interest']\n",
    "        df_eng = merged_result.copy()\n",
    "        df_eng['Discount Percent'] = df_eng['Discount Range']-df_eng['Price Range']\n",
    "        df_eng['Total Sold'] = scaler.fit_transform(df_eng[['Total Sold']])\n",
    "        df_eng['High Interest'] = scaler.fit_transform(df_eng[['Favorite']])\n",
    "\n",
    "# Conditions\n",
    "        df_eng['Highly Rated'] = df_eng['Current Rating'].astype(float).map(lambda x: True if x>df_eng['Current Rating'].mean() else False)\n",
    "        df_eng['Discounted'] = df_eng['Discount Percent'].astype(float).map(lambda x: True if x>0.03 else False)\n",
    "        df_eng['Top Selling'] = df_eng['Total Sold'].map(lambda x: True if x>df_eng['Total Sold'].mean() else False)\n",
    "        df_eng['High Interest'] = df_eng['High Interest'].map(lambda x: True if x>df_eng['High Interest'].mean() else False)\n",
    "\n",
    "# New Columns\n",
    "        df_eng['Trusted'] = df_eng.apply(lambda x: x['Preferred'] | x['Mall'], axis=1)\n",
    "\n",
    "        model_features = ['Price Range']\n",
    "        scoring_criteria = ['Trusted', 'Highly Rated', 'Discounted', 'Top Selling', 'High Interest']\n",
    "\n",
    "        if df_eng.shape[0] < 10:\n",
    "            prd_list = df_eng.sample(n=df_eng.shape[0])\n",
    "        else:\n",
    "            prd_list = df_eng.sample(n=10)\n",
    "        prd_list['Relevance'] = prd_list['Distance']\n",
    "        prd_list['score'] = prd_list['Relevance']\n",
    "\n",
    "        scored_list = prd_list[prd_list['Current Rating'] > 3.8]\n",
    "\n",
    "# Scoring System\n",
    "        trusted_bias = 0.05\n",
    "        highly_rated_bias = 0.05\n",
    "        discounted_bias = 0.05\n",
    "        top_selling_bias = 0.05\n",
    "        high_interest_bias = 0.05\n",
    "\n",
    "        scored_list['score'] = scored_list.apply(lambda x: x['score']-trusted_bias if x['Trusted'] == True else x['score'], axis=1)\n",
    "        scored_list['score'] = scored_list.apply(lambda x: x['score']-highly_rated_bias if x['Highly Rated'] == True else x['score'], axis=1)\n",
    "        scored_list['score'] = scored_list.apply(lambda x: x['score']-discounted_bias if x['Discounted'] == True else x['score'], axis=1)\n",
    "        scored_list['score'] = scored_list.apply(lambda x: x['score']-top_selling_bias if x['Top Selling'] == True else x['score'], axis=1)\n",
    "        scored_list['score'] = scored_list.apply(lambda x: x['score']-high_interest_bias if x['High Interest'] == True else x['score'], axis=1)\n",
    "        scored_list['Performance'] = scored_list['score']\n",
    "        scored_list2 = scored_list[['Product Name', 'Distance', 'Trusted', 'Highly Rated', 'Discounted', 'Top Selling', 'High Interest', 'Performance']].sort_values(by=['Performance'], ascending=True)\n",
    "        st.table(scored_list2)\n",
    "        \n",
    "        nearest = scored_list\n",
    "        \n",
    "        st.write('''**Top-selling Products:**''', unsafe_allow_html=True)\n",
    "        top_sellers = pd.DataFrame(nearest['Total Sold'].sort_values(ascending=False)[:3].reset_index())\n",
    "        top_sellers = top_sellers['index'].tolist()\n",
    "\n",
    "        for i in top_sellers:\n",
    "            st.table(nearest.loc[[i]][['Product Name']])\n",
    "\n",
    "        st.write('''**Most Popular Products:**''', unsafe_allow_html=True)\n",
    "        top_popular = pd.DataFrame(nearest['Favorite'].sort_values(ascending=False)[:3].reset_index())\n",
    "        top_popular = top_popular['index'].tolist()\n",
    "\n",
    "        for i in top_popular:\n",
    "            st.table(nearest.loc[[i]][['Product Name']])\n",
    "        \n",
    "        st.write('''**Top Discounted Products:**''', unsafe_allow_html=True)\n",
    "        top_discount = pd.DataFrame(nearest['Discount Range'].sort_values(ascending=False)[:3].reset_index())\n",
    "        top_discount = top_discount['index'].tolist()\n",
    "\n",
    "        for i in top_discount:\n",
    "            st.table(nearest.loc[[i]][['Product Name']])\n",
    "        \n",
    "        st.write('''**Check out this stores!**''', unsafe_allow_html=True)\n",
    "        top_related = nearest[(nearest['Mall']==True) | (nearest['Preferred']==True)]['Store Name'].unique()[:3]\n",
    "        top_related = pd.DataFrame(top_related)\n",
    "        top_related = top_related.rename(columns={0: \"Store Name\"})\n",
    "        st.table(top_related['Store Name'])\n",
    "\n",
    "        recommend = selected_category['Sub Category 2'][knn_model_result[1][0][0:]].tolist()\n",
    "        counter = Counter(recommend)\n",
    "\n",
    "        to_recommend = {k: v for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)}\n",
    "        to_recommend = [x for x in to_recommend.keys() if str(x) != 'nan']\n",
    "\n",
    "        st.write('''**You might also like:**''', unsafe_allow_html=True)\n",
    "        for i in to_recommend:\n",
    "            st.write(i)\n",
    "            st.table(overall_data[overall_data['Sub Category 2'] == i]['Product Name'].sample(n=1, replace=True))\n",
    "            \n",
    "    if st.button('Random'):\n",
    "        df_random = overall_data.loc[(overall_data['Total Sold'] > overall_data['Total Sold'].mean()) | (overall_data['Current Rating']>overall_data['Current Rating'].mean()) | ((overall_data['Preferred'] ==True) | (overall_data['Mall'] ==True))]\n",
    "        main_cat_list = Counter(df_random['Main Category'].tolist())\n",
    "        main_cat_list= [x for x in main_cat_list.keys() if str(x) != 'nan']\n",
    "        for main_cat in main_cat_list:\n",
    "            st.subheader(main_cat)\n",
    "            st.table(df_random[df_random['Main Category'] ==  main_cat]['Product Name'].sample(n=3, replace=True))\n",
    "\n",
    "elif add_selectbox == 'Recommendations':\n",
    "    st.subheader('Recommendations')\n",
    "    \n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Consider a hybrid filtering approach to the recommender system.</li>\"\\\n",
    "                \"<BLOCKQUOTE><li>Transactions data is needed</li></BLOCKQUOTE>\"\\\n",
    "                \"<li>Sentiment analysis on comments because ratings arenâ€™t accurate.</li>\"\\\n",
    "                \"<li>Utilize spacy linguistic features to filter out results.</li>\"\\\n",
    "                \"<li>Consider making a third-party wishlist site that gathers data from other e-commerce marketplaces.</li>\"\\\n",
    "                \"<li>Save information to neo4j or MongoDB, a graph theory database.</li>\"\\\n",
    "                 \"</ul>\", unsafe_allow_html=True)\n",
    "            \n",
    "else:\n",
    "    st.subheader('Contributors')\n",
    "    st.write('___')\n",
    "    \n",
    "    with open(\"style_img.css\") as f:\n",
    "        st.markdown('<style>{}</style>'.format(f.read()), unsafe_allow_html=True)\n",
    "    \n",
    "    image = Image.open('img/dan.png')\n",
    "    st.image(image, width = 300)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Danilo Gubaton Jr.**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/dcgubatonjr/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    st.write('___')\n",
    "    \n",
    "    image = Image.open('img/emer.png')\n",
    "    st.image(image, width = 300)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Fili Emerson Chua**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/fili-emerson-chua/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    st.write('___')\n",
    "\n",
    "    image = Image.open('img/ran.png')\n",
    "    st.image(image, width = 300)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Rhey Ann Magcalas**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/rhey-magcalas-47541490/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    st.write('___')\n",
    "    \n",
    "    image = Image.open('img/rob.png')\n",
    "    st.image(image, width = 300)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Roberto BaÃ±adera Jr.**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/robertobanaderajr/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    st.write(\"___\")\n",
    "    st.subheader('Special Acknowledgments:')\n",
    "    st.write(\"A big shout-out to these people for helping us make this project possible!\")\n",
    "    st.write(\"___\")\n",
    "    \n",
    "    image = Image.open('img/bash.png')\n",
    "    st.image(image, width = 200)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Albert Yumol**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/albertyumol/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    image = Image.open('img/elissa.png')\n",
    "    st.image(image, width = 200)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**Elissa Cabal**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/elissa-cabal-9790a3141/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "    \n",
    "    image = Image.open('img/jawn.png')\n",
    "    st.image(image, width = 200)\n",
    "    \n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;'>**John Barrion**</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)\n",
    "\n",
    "    st.markdown(\n",
    "    \"\"\"<a style='display: block; text-align: center;' href=\"https://www.linkedin.com/in/johnbarrion/\">LinkedIn</a>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
